{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ffa188d",
   "metadata": {},
   "source": [
    "# DER系アプローチの分析用ノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea111c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, json, re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd2a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例: 物理GPU1番だけを見せる\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454ab6d",
   "metadata": {},
   "source": [
    "## 1) Project root / config / phase 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db36280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: der-mu\n",
      "dataset   : cifar100\n",
      "device    : [device(type='cuda', index=0)]\n"
     ]
    }
   ],
   "source": [
    "# === プロジェクトルートの指定 ===\n",
    "# Notebook を CIDL-main の直下で開いているなら \".\" で OK\n",
    "PROJECT_ROOT = Path(\"/home/kouyou/ContinualLearning/repexp/PyCIL\").resolve()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from trainer import _set_device  # 既存のヘルパをそのまま使う\n",
    "from utils.data_manager import DataManager\n",
    "from utils import factory\n",
    "\n",
    "# === 使いたい設定ファイルと、どの phase を可視化するか ===\n",
    "CONFIG_PATH = \"exps/der_mu/baseline0/cifar100.json\"   # 適宜変更\n",
    "PHASE_ID    = 5                                         # ex) タスク3終了時のモデル → phase3.pkl\n",
    "\n",
    "# === json 読み込み → args にする ===\n",
    "with open(CONFIG_PATH) as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "# device を training と同じ形式 (list of torch.device) に変換\n",
    "_set_device(args)\n",
    "\n",
    "print(\"model_name:\", args[\"model_name\"])\n",
    "print(\"dataset   :\", args[\"dataset\"])\n",
    "print(\"device    :\", args[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bde2c",
   "metadata": {},
   "source": [
    "## 2) Checkpoint path を trainer と同じ規則で組み立てる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66cb90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_dir : logs/der-mu/baseline0/cifar100/0/10/reproduce_1993_resnet32/\n",
      "ckpt_path: logs/der-mu/baseline0/cifar100/0/10/reproduce_1993_resnet32//phase_5.pkl\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "def _seed_to_str(seed):\n",
    "    # trainer.py は args['seed'] をそのまま format しているので、list/int 両対応にしておく\n",
    "    if isinstance(seed, (list, tuple)):\n",
    "        return str(seed[0]) if len(seed) else \"0\"\n",
    "    return str(seed)\n",
    "\n",
    "def build_ckpt_dir(args, root=PROJECT_ROOT / \"checkpoint\"):\n",
    "    \n",
    "    \"\"\"trainer / BaseLearner.save_checkpoint と同じ規則で checkpoint ディレクトリを作る\"\"\"\n",
    "    init_cls = 0 if args [\"init_cls\"] == args[\"increment\"] else args[\"init_cls\"]\n",
    "    log = \"baseline\" if \"log\" not in args else args[\"log\"]\n",
    "    \n",
    "    ckpt_dir = \"logs/{}/{}/{}/{}/{}/{}_{}_{}/\".format(\n",
    "        args[\"model_name\"],\n",
    "        log,\n",
    "        args[\"dataset\"],\n",
    "        init_cls,\n",
    "        args[\"increment\"],\n",
    "        args[\"prefix\"], args[\"seed\"][0], args[\"convnet_type\"],\n",
    "    )\n",
    "    return ckpt_dir\n",
    "\n",
    "ckpt_dir  = build_ckpt_dir(args)\n",
    "ckpt_path = f\"{ckpt_dir}/phase_{PHASE_ID}.pkl\"\n",
    "\n",
    "print(\"ckpt_dir :\", ckpt_dir)\n",
    "print(\"ckpt_path:\", ckpt_path)\n",
    "\n",
    "# # もし exists が False のときは、pattern で探す fallback も書いておくと楽\n",
    "# if not ckpt_path.exists():\n",
    "#     cand = glob(str(ckpt_dir / f\"phase{PHASE_ID}*.pkl\"))\n",
    "#     print(\"fallback candidates:\", cand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9afb7",
   "metadata": {},
   "source": [
    "## 3) DataManager と model を作って checkpoint をロード（DER/TagFex など拡張系対応）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8298965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-7c2c8aada76e>:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=model._device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded. convnets = 6\n"
     ]
    }
   ],
   "source": [
    "def infer_convnet_count_from_state_dict(state_dict):\n",
    "    # keys like: 'convnets.0.xxx', 'convnets.1.xxx', ...\n",
    "    idxs = []\n",
    "    pat = re.compile(r\"^convnets\\.(\\d+)\\.\")\n",
    "    for k in state_dict.keys():\n",
    "        m = pat.match(k)\n",
    "        if m:\n",
    "            idxs.append(int(m.group(1)))\n",
    "    return (max(idxs) + 1) if idxs else 0\n",
    "\n",
    "def classes_at_task(k, args, total_classnum):\n",
    "    # task k の終了時点での total_classes（BaseLearnerの挙動に合わせる）\n",
    "    init_cls = args[\"init_cls\"]\n",
    "    inc = args[\"increment\"]\n",
    "    num = init_cls + k * inc\n",
    "    return min(num, total_classnum)\n",
    "\n",
    "def build_network_skeleton_for_ckpt(model, state_dict, data_manager, args, phase_id):\n",
    "    \"\"\"state_dict をロード可能な形に network を拡張しておく。\n",
    "    DERNet/TagFexNet 系は update_fc を task 回数ぶん呼ぶ必要がある。\n",
    "    \"\"\"\n",
    "    net = model._network\n",
    "    # DataParallel だと update_fc が面倒なので notebook では使わない前提\n",
    "    if isinstance(net, torch.nn.DataParallel):\n",
    "        net = net.module\n",
    "        model._network = net\n",
    "\n",
    "    convnet_count = infer_convnet_count_from_state_dict(state_dict)\n",
    "    if convnet_count == 0:\n",
    "        # ふつうはありえないが、念のため\n",
    "        convnet_count = phase_id + 1\n",
    "\n",
    "    # 最終クラス数は fc.weight の out_features で確定できる\n",
    "    if \"fc.weight\" in state_dict:\n",
    "        final_classes = state_dict[\"fc.weight\"].shape[0]\n",
    "    else:\n",
    "        # fallback\n",
    "        final_classes = classes_at_task(phase_id, args, data_manager.get_total_classnum())\n",
    "\n",
    "    # すでに update_fc 済みのモデルなら二重に増やさない\n",
    "    existing = len(getattr(net, \"convnets\", [])) if hasattr(net, \"convnets\") else 0\n",
    "\n",
    "    # DER/TagFex 系は convnets を持つ。ここが最重要分岐。\n",
    "    if hasattr(net, \"update_fc\") and hasattr(net, \"convnets\"):\n",
    "        # 必要な回数だけ update_fc を呼ぶ\n",
    "        # task k でのクラス数を与えながら進める（最後だけ final_classes で整合）\n",
    "        total_cls = data_manager.get_total_classnum()\n",
    "        for k in range(existing, convnet_count):\n",
    "            nb = classes_at_task(k, args, total_cls)\n",
    "            # 最後の update_fc は checkpoint の fc 次元に合わせる\n",
    "            if k == convnet_count - 1:\n",
    "                nb = final_classes\n",
    "            net.update_fc(nb)\n",
    "    else:\n",
    "        # 拡張なしモデルは update_fc 1回で十分なことが多い\n",
    "        if hasattr(net, \"update_fc\") and \"fc.weight\" in state_dict:\n",
    "            net.update_fc(state_dict[\"fc.weight\"].shape[0])\n",
    "\n",
    "    return net\n",
    "\n",
    "# --- DataManager & model ---\n",
    "data_manager = DataManager(\n",
    "    dataset_name=args[\"dataset\"],\n",
    "    shuffle=args.get(\"shuffle\", True),\n",
    "    seed=int(_seed_to_str(args.get(\"seed\", 0))),\n",
    "    init_cls=args[\"init_cls\"],\n",
    "    increment=args[\"increment\"],\n",
    ")\n",
    "\n",
    "model = factory.get_model(args[\"model_name\"], args)\n",
    "model._device = args[\"device\"][0]\n",
    "model._network.to(model._device)\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=model._device)\n",
    "state_dict = ckpt[\"model_state_dict\"]\n",
    "\n",
    "# 1) skeleton を作る（update_fc を必要回数）\n",
    "build_network_skeleton_for_ckpt(model, state_dict, data_manager, args, PHASE_ID)\n",
    "\n",
    "# 2) state_dict ロード\n",
    "model._network.load_state_dict(state_dict, strict=True)\n",
    "model._network.to(model._device)\n",
    "\n",
    "# 3) 追加情報（あれば）\n",
    "if \"forget_classes\" in ckpt and hasattr(model, \"forget_classes\"):\n",
    "    model.forget_classes = ckpt[\"forget_classes\"]\n",
    "if \"protos\" in ckpt:\n",
    "    model._protos = ckpt[\"protos\"]\n",
    "\n",
    "model._network.eval()\n",
    "print(\"loaded. convnets =\", len(getattr(model._network, \"convnets\", [])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97b99f",
   "metadata": {},
   "source": [
    "## 4) seen / forget / retain クラス集合を作る（MU系なら forget_classes を活用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6859f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 60\n",
      "forget: []\n",
      "retain (head): [0 1 2 3 4 5 6 7 8 9]  ... total 60\n"
     ]
    }
   ],
   "source": [
    "# checkpoint phase 時点での「見えている」クラス数\n",
    "num_classes = classes_at_task(PHASE_ID, args, data_manager.get_total_classnum())\n",
    "all_seen = np.arange(num_classes)\n",
    "\n",
    "forget_set = set(getattr(model, \"forget_classes\", []))\n",
    "forget = np.array(sorted([c for c in forget_set if c < num_classes]), dtype=int)\n",
    "retain = np.setdiff1d(all_seen, forget)\n",
    "\n",
    "print(\"num_classes:\", num_classes)\n",
    "print(\"forget:\", forget)\n",
    "print(\"retain (head):\", retain[:10], \" ... total\", len(retain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10b5ea",
   "metadata": {},
   "source": [
    "## 5) Manual forget class 指定（要件2）と unlearn pruning 設定\n",
    "\n",
    "- このセクション以降は **読み込んだ学習済みモデルに対して unlearning 処理だけ**を行います（要件1）。\n",
    "- 忘却クラスはここで **手動で指定**します（要件2）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66776553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 60\n",
      "FORGET_CLASSES: [50, 51]\n",
      "retain classes: 58\n",
      "TARGET_BACKBONE_ID: 5\n"
     ]
    }
   ],
   "source": [
    "# ===== Manual forget classes (EDIT HERE) =====\n",
    "# 例: task5 に含まれる忘却クラス c を指定\n",
    "FORGET_CLASSES = [50,51]   # <-- ここを手で変更\n",
    "\n",
    "# ===== Target backbone selection =====\n",
    "# 基本は「forget class を含むタスクを直接学習した backbone」を対象にする\n",
    "TARGET_BACKBONE_ID = PHASE_ID  # task t の backbone を使うなら通常これでOK（必要なら手動で変更）\n",
    "\n",
    "# ===== Unlearn pruning hyperparams =====\n",
    "LAMBDA_RETAIN = 1.0      # S = I_forget - lambda * I_retain の lambda\n",
    "PRUNE_RATIO   = 0.005    # 上位何割を壊すか（まずは小さめ推奨）\n",
    "MAX_BATCHES_F = 500      # Fisher推定に使う最大バッチ数（計算節約）\n",
    "MAX_BATCHES_R = 25000\n",
    "BATCH_SIZE    = 256\n",
    "NUM_WORKERS   = 4\n",
    "\n",
    "# ===== Recompute seen/forget/retain sets (override) =====\n",
    "num_classes = classes_at_task(PHASE_ID, args, data_manager.get_total_classnum())\n",
    "all_seen = np.arange(num_classes)\n",
    "\n",
    "forget = np.array(sorted([c for c in FORGET_CLASSES if c < num_classes]), dtype=int)\n",
    "retain = np.setdiff1d(all_seen, forget)\n",
    "\n",
    "print(\"num_classes:\", num_classes)\n",
    "print(\"FORGET_CLASSES:\", forget.tolist())\n",
    "print(\"retain classes:\", len(retain))\n",
    "print(\"TARGET_BACKBONE_ID:\", TARGET_BACKBONE_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11dcedd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpers ready.\n"
     ]
    }
   ],
   "source": [
    "# ===== Helpers: device / logits / backbone access =====\n",
    "def _unwrap_net(net):\n",
    "    # DataParallel / DDP を剥がす\n",
    "    if hasattr(net, \"module\"):\n",
    "        return net.module\n",
    "    return net\n",
    "\n",
    "def _get_device_from_model(model):\n",
    "    dev = getattr(model, \"_device\", None)\n",
    "    if isinstance(dev, (list, tuple)) and len(dev):\n",
    "        return dev[0]\n",
    "    if isinstance(dev, torch.device):\n",
    "        return dev\n",
    "    # fallback: network parameter device\n",
    "    return next(model._network.parameters()).device\n",
    "\n",
    "def _get_logits_from_network_output(out):\n",
    "    # DER系は dict を返すことが多い: {\"logits\": ...}\n",
    "    if isinstance(out, dict):\n",
    "        if \"logits\" in out:\n",
    "            return out[\"logits\"]\n",
    "        if \"logit\" in out:\n",
    "            return out[\"logit\"]\n",
    "    # それ以外はテンソルそのものだとみなす\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_accuracy(model, loader, allowed_classes=None):\n",
    "    model._network.eval()\n",
    "    dev = _get_device_from_model(model)\n",
    "    correct, total = 0, 0\n",
    "    for _, x, y in loader:\n",
    "        x = x.to(dev, non_blocking=True)\n",
    "        y = y.to(dev, non_blocking=True)\n",
    "        out = model._network(x)\n",
    "        logits = _get_logits_from_network_output(out)\n",
    "        if allowed_classes is not None:\n",
    "            logits = logits[:, allowed_classes]\n",
    "            pred = logits.argmax(dim=1)\n",
    "            pred = torch.as_tensor(allowed_classes, device=dev)[pred]\n",
    "        else:\n",
    "            pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def get_target_backbone(net, bb_id: int):\n",
    "    net = _unwrap_net(net)\n",
    "    # DERNet: net.convnets[bb_id]\n",
    "    if hasattr(net, \"convnets\"):\n",
    "        return net.convnets[bb_id]\n",
    "    # TagFex系など: net.backbones / net.nets 等の可能性\n",
    "    for attr in [\"backbones\", \"nets\", \"models\", \"encoders\"]:\n",
    "        if hasattr(net, attr):\n",
    "            obj = getattr(net, attr)\n",
    "            try:\n",
    "                return obj[bb_id]\n",
    "            except Exception:\n",
    "                pass\n",
    "    raise AttributeError(\"backbone list (e.g., net.convnets) not found. Please check your network structure.\")\n",
    "\n",
    "def build_class_loader(data_manager, class_indices, source=\"train\", mode=\"train\",\n",
    "                       batch_size=128, num_workers=4, shuffle=True):\n",
    "    idx = class_indices.tolist() if isinstance(class_indices, np.ndarray) else list(class_indices)\n",
    "    # PyCIL DataManager は実装差があるので、keyword/positional両対応にする\n",
    "    try:\n",
    "        ds = data_manager.get_dataset(indices=idx, source=source, mode=mode)\n",
    "    except TypeError:\n",
    "        ds = data_manager.get_dataset(idx, source, mode)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(\"helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d2ab5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlearn pruning core ready.\n"
     ]
    }
   ],
   "source": [
    "# ===== Unlearn pruning core =====\n",
    "def fisher_diag_on_loader(model, module, loader, max_batches=50):\n",
    "    \"\"\"\n",
    "    module: 対象backbone（ここだけをスコアリングする）\n",
    "    返り値: dict[name] = fisher_diag (CPU tensor)\n",
    "    \"\"\"\n",
    "    dev = _get_device_from_model(model)\n",
    "    model._network.eval()\n",
    "    # 勾配を取るので no_grad は使わない\n",
    "    fisher = {}\n",
    "    for n, p in module.named_parameters():\n",
    "        # print(n)\n",
    "        if p.requires_grad:\n",
    "            fisher[n] = torch.zeros_like(p, device=\"cpu\")\n",
    "            print(\"fisher[n].shape: \", fisher[n].shape)\n",
    "\n",
    "    nb = 0\n",
    "    for _, x, y in loader:\n",
    "        x = x.to(dev, non_blocking=True)\n",
    "        y = y.to(dev, non_blocking=True)\n",
    "\n",
    "        # 既存gradクリア\n",
    "        model._network.zero_grad(set_to_none=True)\n",
    "\n",
    "        out = model._network(x)\n",
    "        logits = _get_logits_from_network_output(out)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        for n, p in module.named_parameters():\n",
    "            if (not p.requires_grad) or (p.grad is None):\n",
    "                continue\n",
    "            fisher[n] += (p.grad.detach().cpu() ** 2)\n",
    "\n",
    "        nb += 1\n",
    "        if max_batches is not None and nb >= max_batches:\n",
    "            break\n",
    "\n",
    "    for n in fisher:\n",
    "        fisher[n] /= max(nb, 1)\n",
    "    return fisher\n",
    "\n",
    "def compute_unlearn_score(fisher_forget, fisher_retain, lambda_retain=1.0):\n",
    "    # S = I_forget - lambda * I_retain （CPU tensor dict）\n",
    "    score = {}\n",
    "    for n in fisher_forget:\n",
    "        if n in fisher_retain:\n",
    "            score[n] = fisher_forget[n] - lambda_retain * fisher_retain[n]\n",
    "        else:\n",
    "            score[n] = fisher_forget[n].clone()\n",
    "    return score\n",
    "\n",
    "def apply_pruning_by_score(module, score, prune_ratio=0.01, only_ndim_ge2=True):\n",
    "    \"\"\"\n",
    "    score: dict[name] -> CPU tensor (same shape as param)\n",
    "    prune_ratio: 上位何割の score（正の部分）をゼロ化\n",
    "    only_ndim_ge2: conv/linear weight を主に対象にする（BNの1Dは除外）\n",
    "    返り値: mask_dict, stats\n",
    "    \"\"\"\n",
    "    # まず対象スコアを1本に集約\n",
    "    all_scores = []\n",
    "    meta = []\n",
    "    for n, p in module.named_parameters():\n",
    "        if n not in score:\n",
    "            continue\n",
    "        if only_ndim_ge2 and p.ndim < 2:\n",
    "            continue\n",
    "        s = score[n]\n",
    "        s_pos = torch.clamp(s, min=0.0)  # forget重要（正）だけを対象にする\n",
    "        all_scores.append(s_pos.flatten())\n",
    "        meta.append(n)\n",
    "\n",
    "    if len(all_scores) == 0:\n",
    "        raise RuntimeError(\"No parameters selected for pruning. (Check only_ndim_ge2 or module structure.)\")\n",
    "\n",
    "    all_scores_cat = torch.cat(all_scores)\n",
    "    k = int(prune_ratio * all_scores_cat.numel())\n",
    "    if k <= 0:\n",
    "        raise RuntimeError(f\"prune_ratio too small: {prune_ratio} (k=0)\")\n",
    "\n",
    "    # threshold: 上位k個に入る最小値\n",
    "    topk = torch.topk(all_scores_cat, k=k, largest=True)\n",
    "    thr = topk.values.min().item()\n",
    "\n",
    "    mask_dict = {}\n",
    "    total_elems = 0\n",
    "    pruned_elems = 0\n",
    "\n",
    "    for n, p in module.named_parameters():\n",
    "        if n not in score:\n",
    "            continue\n",
    "        if only_ndim_ge2 and p.ndim < 2:\n",
    "            continue\n",
    "        s_pos = torch.clamp(score[n], min=0.0)\n",
    "        m = (s_pos >= thr)  # CPU bool\n",
    "        # apply\n",
    "        m_dev = m.to(p.device)\n",
    "        with torch.no_grad():\n",
    "            p.data[m_dev] = 0.0\n",
    "\n",
    "        mask_dict[n] = m  # keep on CPU\n",
    "        total_elems += m.numel()\n",
    "        pruned_elems += int(m.sum().item())\n",
    "\n",
    "    stats = {\n",
    "        \"threshold\": thr,\n",
    "        \"total_elems\": total_elems,\n",
    "        \"pruned_elems\": pruned_elems,\n",
    "        \"pruned_ratio_actual\": pruned_elems / max(total_elems, 1),\n",
    "    }\n",
    "    return mask_dict, stats\n",
    "\n",
    "print(\"unlearn pruning core ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca17d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEFORE] acc_forget=0.9680  acc_retain=0.8749\n",
      "fisher[n].shape:  torch.Size([16, 3, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([32, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([64, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([10, 64])\n",
      "fisher[n].shape:  torch.Size([10])\n",
      "fisher[n].shape:  torch.Size([16, 3, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([16])\n",
      "fisher[n].shape:  torch.Size([32, 16, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([32])\n",
      "fisher[n].shape:  torch.Size([64, 32, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64, 64, 3, 3])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([64])\n",
      "fisher[n].shape:  torch.Size([10, 64])\n",
      "fisher[n].shape:  torch.Size([10])\n",
      "[PRUNE] {'threshold': 0.0001231449714396149, 'total_elems': 461872, 'pruned_elems': 2309, 'pruned_ratio_actual': 0.004999220563272942}\n",
      "[AFTER ] acc_forget=0.1150  acc_retain=0.8446\n",
      "saved: logs/der-mu/baseline0/cifar100/0/10/reproduce_1993_resnet32/phase_5_unlearn_pruned[50, 51].pth\n"
     ]
    }
   ],
   "source": [
    "# ===== 6) Build forget/retain loaders (train split) =====\n",
    "# 注意: unlearning のため、通常は train split を使う（test を使う場合はここを変える）\n",
    "\n",
    "forget_loader = build_class_loader(\n",
    "    data_manager, forget, source=\"train\", mode=\"test\",\n",
    "    batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True\n",
    ")\n",
    "\n",
    "# retain は全部だと重いので、必要なら一部クラスだけに絞る／サンプル数を制限する\n",
    "retain_loader = build_class_loader(\n",
    "    data_manager, retain, source=\"train\", mode=\"train\",\n",
    "    batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True\n",
    ")\n",
    "\n",
    "# ===== 7) Evaluate BEFORE pruning =====\n",
    "acc_forget_before = eval_accuracy(model, forget_loader)\n",
    "acc_retain_before = eval_accuracy(model, retain_loader)\n",
    "\n",
    "print(f\"[BEFORE] acc_forget={acc_forget_before:.4f}  acc_retain={acc_retain_before:.4f}\")\n",
    "\n",
    "# ===== 8) Compute Fisher (forget vs retain) on TARGET_BACKBONE =====\n",
    "target_bb = get_target_backbone(model._network, TARGET_BACKBONE_ID)\n",
    "\n",
    "# Fisher 推定のために grad を有効化（freezeされていてもここだけスコアリング/編集したい）\n",
    "for p in target_bb.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "f_f = fisher_diag_on_loader(model, target_bb, forget_loader, max_batches=MAX_BATCHES_F)\n",
    "f_r = fisher_diag_on_loader(model, target_bb, retain_loader, max_batches=MAX_BATCHES_R)\n",
    "\n",
    "score = compute_unlearn_score(f_f, f_r, lambda_retain=LAMBDA_RETAIN)\n",
    "\n",
    "# ===== 9) Apply pruning =====\n",
    "mask_dict, prune_stats = apply_pruning_by_score(\n",
    "    target_bb, score, prune_ratio=PRUNE_RATIO, only_ndim_ge2=True\n",
    ")\n",
    "\n",
    "print(\"[PRUNE]\", prune_stats)\n",
    "\n",
    "# ===== 10) Evaluate AFTER pruning =====\n",
    "acc_forget_after = eval_accuracy(model, forget_loader)\n",
    "acc_retain_after = eval_accuracy(model, retain_loader)\n",
    "\n",
    "print(f\"[AFTER ] acc_forget={acc_forget_after:.4f}  acc_retain={acc_retain_after:.4f}\")\n",
    "\n",
    "# ===== Optional: save the pruned model checkpoint (weights only) =====\n",
    "SAVE_PRUNED = True\n",
    "if SAVE_PRUNED:\n",
    "    out_path = Path(str(ckpt_path)).with_suffix(\"\").as_posix() + f\"_unlearn_pruned{FORGET_CLASSES}.pth\"\n",
    "    torch.save({\n",
    "        \"state_dict\": _unwrap_net(model._network).state_dict(),\n",
    "        \"forget_classes\": forget.tolist(),\n",
    "        \"phase_id\": PHASE_ID,\n",
    "        \"target_backbone_id\": TARGET_BACKBONE_ID,\n",
    "        \"prune_stats\": prune_stats\n",
    "    }, out_path)\n",
    "    print(\"saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4815c93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]],\n",
       "\n",
       "         [[False, False, False],\n",
       "          [False, False, False],\n",
       "          [False, False, False]]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_dict.keys()\n",
    "mask_dict[\"conv_1_3x3.weight\"].shape\n",
    "mask_dict[\"conv_1_3x3.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8e3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7514fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
